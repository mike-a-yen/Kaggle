{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from functools import partial\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "from typing import Callable, List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dirname = Path('../').resolve()\n",
    "raw_data_dirname = project_dirname / 'data/raw'\n",
    "data_files = list(raw_data_dirname.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_df_sha(df: pd.DataFrame) -> str:\n",
    "    id_str = ''.join(map(str, df.id))\n",
    "    id_bstr = id_str.encode()\n",
    "    sha = hashlib.sha256(id_bstr)\n",
    "    return sha.hexdigest()\n",
    "\n",
    "    \n",
    "class RawDataset:\n",
    "    def __init__(self, project_dirname:Path, subsample: int = None) -> None:\n",
    "        self.subsample = subsample\n",
    "        self.project_dirname = project_dirname\n",
    "        self.data_dirname = self.project_dirname / 'data'\n",
    "        self.raw_data_dirname = self.data_dirname / 'raw'\n",
    "        self.train_filename = self.raw_data_dirname / 'train.csv'\n",
    "        self.test_filename = self.raw_data_dirname / 'test.csv'\n",
    "        \n",
    "        assert self.train_filename.exists()\n",
    "        assert self.test_filename.exists()\n",
    "        \n",
    "        self.train_df = pd.read_csv(self.train_filename)\n",
    "        self.test_df = pd.read_csv(self.test_filename)\n",
    "        \n",
    "        if self.subsample != 0:\n",
    "            self.train_df = self.train_df.iloc[0: self.subsample]\n",
    "        self.identifier = compute_df_sha(self.train_df)\n",
    "\n",
    "class ProcessedDataset:\n",
    "    toxicity_subtypes = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    identity_attributes = [\n",
    "        'male', 'female', 'transgender', 'other_gender',\n",
    "        'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation',\n",
    "        'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion',\n",
    "        'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
    "        'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
    "        'other_disability'\n",
    "    ]\n",
    "    _binarize_columns = ['target'] + toxicity_subtypes\n",
    "    binary_columns = [f'b_{name}' for name in _binarize_columns]\n",
    "    \n",
    "    def __init__(self, raw_dataset: RawDataset, overwrite: bool = False) -> None:\n",
    "        self.identifier = raw_dataset.identifier\n",
    "        self.project_dirname = raw_dataset.project_dirname\n",
    "        self.data_dirname = raw_dataset.data_dirname\n",
    "        self.cache_path = self.data_dirname / f'{self.identifier}.pklb'\n",
    "        \n",
    "        if self.cache_path.exists() and not overwrite:\n",
    "            print(f'Loading processed dataset from {self.cache_path}')\n",
    "            self.load()\n",
    "        else:\n",
    "            self.train_df = raw_dataset.train_df\n",
    "            self.test_df = raw_dataset.test_df\n",
    "            self.featurize()\n",
    "            self.save()\n",
    "\n",
    "    def featurize(self) -> None:\n",
    "        print('Featurizing....')\n",
    "        self._prepare_df_labels(self.train_df)\n",
    "        self._prepare_features(self.train_df)\n",
    "        self._prepare_features(self.test_df)\n",
    "            \n",
    "    def _prepare_df_labels(self, df: pd.DataFrame) -> None:\n",
    "        for column, new_column in zip(self._binarize_columns, self.binary_columns):\n",
    "            df[new_column] = df[column].apply(self._binarize_label)\n",
    "\n",
    "    def _prepare_features(self, df: pd.DataFrame) -> None:\n",
    "        doc = df.comment_text.apply(tokenizer)\n",
    "        df['comment_words'] = doc.apply(lambda x: [w.text for w in x])\n",
    "        \n",
    "    def _binarize_label(self, target: float) -> int:\n",
    "        \"\"\"According to competition rules, target values >= 0.5 are considered the positive class.\"\"\"\n",
    "        return int(target >= 0.5)\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        cache_data = {\n",
    "            'train_df': self.train_df,\n",
    "            'test_df': self.test_df\n",
    "        }\n",
    "        with open(self.cache_path, 'wb') as fw:\n",
    "            pickle.dump(cache_data, fw)\n",
    "        \n",
    "    def load(self) -> None:\n",
    "        with open(self.cache_path, 'rb') as fo:\n",
    "            cache_data = pickle.load(fo)\n",
    "        self.train_df = cache_data['train_df']\n",
    "        self.test_df = cache_data['test_df']\n",
    "        \n",
    "    \n",
    "\n",
    "def split_df(df: pd.DataFrame, frac: float = 0.1) -> Tuple[pd.DataFrame]:\n",
    "    n_val = int(np.ceil(df.shape[0]*frac))\n",
    "    df = df.sample(frac=1.)\n",
    "    val_df = df.iloc[0: n_val]\n",
    "    train_df = df.iloc[n_val:]\n",
    "    assert val_df.shape[0] + train_df.shape[0] == df.shape[0]\n",
    "    return train_df, val_df\n",
    "    \n",
    "class TrainableDataset:\n",
    "    def __init__(self, processed_dataset: ProcessedDataset) -> None:\n",
    "        self.trainval_df = processed_dataset.train_df\n",
    "        self.train_df, self.val_df = split_df(self.trainval_df, frac=0.1)\n",
    "        self.test_df = processed_dataset.test_df\n",
    "        \n",
    "        self.n_train = self.train_df.shape[0]\n",
    "        self.n_val = self.val_df.shape[0]\n",
    "        self.n_test = self.test_df.shape[0]\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Train samples: {self.n_train}, Val samples: {self.n_val}, Test samples: {self.n_test}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD = \"<PAD>\"\n",
    "    BOS = \"<BOS>\"\n",
    "    EOS = \"<EOS>\"\n",
    "    BOT = \"<BOT>\"\n",
    "    EOT = \"<EOT>\"\n",
    "    UNK = \"<UNK>\"\n",
    "    \n",
    "    specials = [PAD, BOS, EOS, BOT, EOT, UNK]\n",
    "    vocab = []\n",
    "    token_to_int = dict()\n",
    "    int_to_token = dict()\n",
    "    \n",
    "    def __init__(self, tokens: list = []) -> None:\n",
    "        [self.add_token(t) for t in self.specials]\n",
    "        [self.add_token(t) for t in tokens]\n",
    "        self.UNK_IDX = self.vocab.index(self.UNK)\n",
    "            \n",
    "    def add_token(self, token) -> None:\n",
    "        if token in self.vocab:\n",
    "            return\n",
    "        else:\n",
    "            idx = len(self.vocab)\n",
    "            self.token_to_int[token] = idx\n",
    "            self.int_to_token[idx] = token\n",
    "            self.vocab.append(token)\n",
    "            \n",
    "    def __getitem__(self, token: str) -> int:\n",
    "        return self.token_to_int.get(token, self.UNK_IDX)\n",
    "    \n",
    "    def __contains__(self, token: str) -> bool:\n",
    "        return token in self.vocab\n",
    "    \n",
    "    def get(self, x: Union[str, int], reverse: bool = False) -> int:\n",
    "        if reverse:\n",
    "            return self.int_to_token.get(x, self.UNK)\n",
    "        else:\n",
    "            return self.token_to_int.get(x, self.UNK_IDX)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_to_int)\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self)\n",
    "    \n",
    "    \n",
    "class VocabEncoder:\n",
    "    def __init__(self, vocab: Vocabulary) -> None:\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def _encode_token(self, token: str) -> int:\n",
    "        encoded = []\n",
    "        if Vocabulary.BOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOT])\n",
    "        encoded += [self.vocab[c] for c in token]\n",
    "        if Vocabulary.EOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOT])\n",
    "        return encoded\n",
    "        \n",
    "    def encode(self, seq: List[str]):\n",
    "        encoded = []\n",
    "        if Vocabulary.BOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOS])\n",
    "        for token in seq:\n",
    "            encoded += self._encode_token(token)\n",
    "        if Vocabulary.EOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOS])\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, seq: List[int]) -> List[str]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, X_cols: Union[list, str], Y_cols: Union[list, str] = None) -> None:\n",
    "        self.X_cols = X_cols\n",
    "        self.Y_cols = Y_cols\n",
    "        self.vocab = Vocabulary(string.printable)\n",
    "        self.encoder = VocabEncoder(self.vocab)\n",
    "        assert 'a' in self.vocab\n",
    "\n",
    "    def _column_selector(self, df: pd.DataFrame, columns: Union[list, str]) -> pd.Series:\n",
    "        return df[columns]\n",
    "    \n",
    "    def _vocab_encoder(self, seq: List[str]) -> pd.Series:\n",
    "        return encode_sequence(seq, self.vocab)\n",
    "\n",
    "    def __call__(self, sample: pd.Series) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X = self._column_selector(sample, self.X_cols)\n",
    "        X = self.encoder.encode(X)\n",
    "        # maxlen = max(map(len, X))\n",
    "        # X = pad_sequences(X, maxlen=maxlen, value=self.vocab[self.vocab.PAD])\n",
    "        if self.Y_cols is not None:\n",
    "            Y = self._column_selector(sample, self.Y_cols)\n",
    "        else:\n",
    "            Y = None\n",
    "        X = torch.LongTensor(X)\n",
    "        Y = torch.Tensor([Y])\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "class BatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int, shuffle=False) -> None:\n",
    "        self.N = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def _index_sampler(self) -> None:\n",
    "        if self.shuffle:\n",
    "            self.idxs = torch.randperm(self.N).tolist()\n",
    "        else:\n",
    "            self.idxs = torch.arange(self.N).tolist()\n",
    "        \n",
    "    def __iter__(self) -> List[int]:\n",
    "        self._index_sampler()\n",
    "        for i in range(0, self.N, self.batch_size):\n",
    "            yield self.idxs[i: i+self.batch_size]\n",
    "\n",
    "\n",
    "class CommentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Callable) -> None:\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.df.iloc[idx]\n",
    "        X, Y = self.transform(sample)\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "def collate(batch: List[tuple]) -> Tuple[torch.Tensor]:\n",
    "    X, Y = zip(*batch)\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    Y = torch.stack(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderBunch:\n",
    "    def __init__(self, train_dl: torch.utils.data.DataLoader, val_dl: torch.utils.data.DataLoader, test_dl: torch.utils.data.DataLoader = None, c = None) -> None:\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c\n",
    "    \n",
    "    @property\n",
    "    def train_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.train_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def val_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.val_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def test_ds(self) -> torch.utils.data.Dataset:\n",
    "        if self.test_dl is not None:\n",
    "            return self.test_dl.dataset\n",
    "        else:\n",
    "            raise ValueError('No test dataloader available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, model_params: dict) -> None:\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            model_params['vocab_size'],\n",
    "            model_params['embedding_size'],\n",
    "            padding_idx=model_params['padding_idx'],\n",
    "        )\n",
    "        self.conv_layer = nn.Conv1d(\n",
    "            model_params['embedding_size'],\n",
    "            model_params['d_model'],\n",
    "            model_params['width'],\n",
    "            \n",
    "        )\n",
    "        self.conv_pool_layer = nn.MaxPool1d(\n",
    "            model_params['width'],\n",
    "            stride=model_params['width'] - 1\n",
    "        )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            model_params['d_model'],\n",
    "            model_params['nhead'],\n",
    "            model_params['d_model']\n",
    "        )\n",
    "        self.transformer_pool_layer = nn.AdaptiveMaxPool1d(1)\n",
    "        self.logit_layer = nn.Linear(model_params['d_model'], 1)\n",
    "        self.conv_act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, token_ids) -> torch.Tensor:\n",
    "        emb = self.embedding_layer(token_ids)\n",
    "        emb_channels_first = torch.transpose(emb, 1, 2)\n",
    "        conv = self.conv_layer(emb_channels_first)\n",
    "        conv_pool = self.conv_pool_layer(conv)\n",
    "        conv_pool = torch.transpose(conv_pool, 1, 2)\n",
    "        conv_pool = self.conv_act(conv_pool)\n",
    "        encoded = self.transformer_layer(conv_pool)\n",
    "        encoded = torch.transpose(encoded, 1, 2)\n",
    "        encoded_pool = self.transformer_pool_layer(encoded)\n",
    "        encoded_pool = encoded_pool[..., 0]\n",
    "        logit = self.logit_layer(encoded_pool)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_with_logits(y_hat, y_true):\n",
    "    pred = y_hat >= 0\n",
    "    truth = y_true >= 0.5\n",
    "    acc = (pred==truth).float().mean()\n",
    "    return acc\n",
    "\n",
    "    \n",
    "class Learner:\n",
    "    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.modules.loss, data: DataLoaderBunch) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = list(model.parameters())[0].device\n",
    "        self.data = data\n",
    "        self.recorder = Recorder()\n",
    "        self.callbacks = CallbackHandler([self.recorder])\n",
    "        \n",
    "    def one_batch(self, X: torch.Tensor, Y: torch.Tensor) -> None:\n",
    "        X, Y = self._convert_batch((X, Y))\n",
    "        if not self.callbacks.on_batch_begin(X, Y): return\n",
    "        y_hat = self.model(X)\n",
    "        if not self.callbacks.on_loss_begin(y_hat): return\n",
    "        loss = self.loss_fn(y_hat, Y)\n",
    "        self.callbacks.on_loss_end(loss)\n",
    "        if not self.callbacks.on_backward_begin(): return\n",
    "        loss.backward()  # get grads\n",
    "        self.callbacks.on_backward_end()\n",
    "        self.callbacks.on_step_begin()\n",
    "        self.optimizer.step()  # apply grads\n",
    "        self.callbacks.on_step_end()\n",
    "        self.optimizer.zero_grad()  # zero grads\n",
    "        self.callbacks.on_batch_end()\n",
    "        \n",
    "    def fit_one_epoch(self, epoch: int = None) -> None:\n",
    "        if epoch == 0: self.callbacks.on_fit_begin(self)\n",
    "        self.callbacks.on_epoch_begin(epoch)\n",
    "        with self.pbar(self.data.train_dl) as pbar:\n",
    "            for batch in self.data.train_dl:\n",
    "                self.one_batch(*batch)\n",
    "                batch_size = self.recorder.batch_size\n",
    "                pbar.update(batch_size)\n",
    "        self.callbacks.on_epoch_end()\n",
    "        return \n",
    "                \n",
    "    def validate(self) -> None:\n",
    "        self.callbacks.begin_validate()\n",
    "        with torch.no_grad():\n",
    "            with self.pbar(self.data.val_dl) as pbar:\n",
    "                for batch in self.data.val_dl:\n",
    "                    self.one_batch(*batch)\n",
    "                    batch_size = self.recorder.batch_size\n",
    "                    pbar.update(batch_size)\n",
    "        self.callbacks.end_validate()\n",
    "        return\n",
    "\n",
    "    def pbar(self, dataloader: torch.utils.data.DataLoader, **kwargs) -> tqdm_notebook:\n",
    "        params = {\n",
    "            'total': len(dataloader.dataset),\n",
    "            'unit': 'samples',\n",
    "            'leave': False\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "        return tqdm_notebook(**params)\n",
    "        \n",
    "    \n",
    "    def _convert_batch(self, batch: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"Send batch data to the model's device\"\"\"\n",
    "        return tuple(x.to(self.device) for x in batch)\n",
    "    \n",
    "class Tester(Learner):\n",
    "    def __init__(self, learner: nn.Module, data: DataLoaderBunch) -> None:\n",
    "        self.learner = learner\n",
    "        self.data = data\n",
    "        self.recorder = Recorder()\n",
    "        self.callbacks = CallbackHandler([self.recorder])\n",
    "        self.callbacks.learner = learner\n",
    "        \n",
    "    def evaluate(self) -> None:\n",
    "        self.callbacks.begin_validate()\n",
    "        with torch.no_grad():\n",
    "            with self._pbar(self.data.test_dl) as pbar:\n",
    "                for batch in self.data.test_dl:\n",
    "                    X, Y = self._convert_batch(*batch)\n",
    "                    y_hat = self.learner.model(X)\n",
    "                    batch_size = y_hat.shape[0]\n",
    "                    cost = self.learner.loss_fn(y_hat, Y) * batch_size\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        return True\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_begin(self, epoch) -> bool:\n",
    "        self.epoch = epoch\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "class CallbackHandler:\n",
    "    def __init__(self, callbacks: List[Callback] = []) -> None:\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        self.in_train = True\n",
    "        learner.stop = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_fit_begin(learner)\n",
    "        return res\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        res = not self.in_train\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.after_fit()\n",
    "        return res\n",
    "    \n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        self.learner.model.train()\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_epoch_begin(epoch)\n",
    "        return res\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_epoch_end()\n",
    "        return res\n",
    "        \n",
    "    def begin_validate(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.begin_validate()\n",
    "        return res\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.end_validate()\n",
    "        return res\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.begin_test()\n",
    "        return res\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.end_test()\n",
    "        return res\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_batch_begin(X, Y)\n",
    "        return res\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_loss_begin(y_hat)\n",
    "        return res\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_loss_end(loss)\n",
    "        return res\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_backward_begin()\n",
    "        return res\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_backward_end()\n",
    "        return res\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_step_begin()\n",
    "        return res\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_step_end()\n",
    "        return res\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_batch_end()\n",
    "        return res\n",
    "    \n",
    "    def do_stop(self) -> bool:\n",
    "        try:\n",
    "            return self.learner.stop\n",
    "        finally:\n",
    "            self.learner.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    def __init__(self) -> None:\n",
    "        self.records = dict(\n",
    "            loss=[], val_loss=[], \n",
    "            train_time=[], val_time=[],\n",
    "            train_samples=[], val_samples=[],\n",
    "        )\n",
    "        self.test_records = dict(loss=None, val_loss=None, test_time=None, test_samples=None)\n",
    "\n",
    "    def on_fit_begin(self, learner) -> bool:\n",
    "        super(Recorder, self).on_fit_begin(learner)\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        return True\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        super(Recorder, self).on_epoch_begin(epoch)\n",
    "        self._reset_state()\n",
    "        return True\n",
    "        \n",
    "    def on_batch_begin(self, X, Y) -> bool:\n",
    "        self.batch_size = Y.shape[0]\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss) -> bool:\n",
    "        self.total_loss += loss.item() * self.batch_size\n",
    "        self.total_samples += self.batch_size\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return self.in_train\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        super(Recorder, self).on_epoch_end()\n",
    "        self._log_metrics()\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        super(Recorder, self).begin_validate()\n",
    "        self.in_train = False\n",
    "        self._reset_state()\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_train = True\n",
    "        print(f'{self._display_latest_metrics()}')\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        super(Recorder, self).begin_test()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_test = False\n",
    "        return True\n",
    "    \n",
    "    def _log_metrics(self) -> None:\n",
    "        elapsed_time = time.time() - self.time_start\n",
    "        self.total_loss /= self.total_samples\n",
    "        if self.in_train:\n",
    "            self.records['loss'].append(self.total_loss)\n",
    "            self.records['train_time'].append(elapsed_time)\n",
    "            self.records['train_samples'].append(self.total_samples)\n",
    "        elif self.in_test:\n",
    "            self.records['test_loss'] = self.total_loss\n",
    "            self.records['test_time'] = elapsed_time\n",
    "            self.records['test_samples'] = self.total_samples\n",
    "        else:\n",
    "            self.records['val_loss'].append(self.total_loss)\n",
    "            self.records['val_time'].append(elapsed_time)\n",
    "            self.records['val_samples'].append(self.total_samples)\n",
    "    \n",
    "    def _reset_state(self) -> None:\n",
    "        self.total_loss = 0.\n",
    "        self.total_samples = 0\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "    def _display_latest_metrics(self) -> str:\n",
    "        total_time = int(self.records[\"train_time\"][-1] + self.records[\"val_time\"][-1])\n",
    "        epoch = f'Epoch {self.epoch} ({total_time} sec):'\n",
    "        train_loss = f'loss = {self.records[\"loss\"][-1]:0.5f}'\n",
    "        val_loss = f'val loss = {self.records[\"val_loss\"][-1]:0.5f}'\n",
    "        return f'{epoch} {train_loss} {val_loss}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training callback routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def one_batch(X: torch.Tensor, Y:torch.Tensor, cb: CallbackHandler) -> None:\n",
    "    if not cb.on_batch_begin(X, Y): return\n",
    "    y_hat = cb.learner.model(X)\n",
    "    if not cb.on_loss_begin(y_hat): return\n",
    "    loss = cb.learner.loss_fn(cb.learner.model(X), Y)\n",
    "    cb.on_loss_end(loss)\n",
    "    if not cb.on_backward_begin(): return\n",
    "    loss.backward()\n",
    "    cb.on_backward_end()\n",
    "    cb.on_step_begin()\n",
    "    cb.learner.optimizer.step()\n",
    "    cb.on_step_end()\n",
    "    cb.learner.optimizer.zero_grad()\n",
    "    cb.on_batch_end()\n",
    "        \n",
    "def all_batches(dataloader: torch.utils.data.DataLoader, cb) -> None:\n",
    "    for X, Y in dataloader:\n",
    "        one_batch(X, Y, cb)\n",
    "        if cb.do_stop(): return\n",
    "        \n",
    "def fit(epochs: int, learner: Learner, cb: CallbackHandler) -> None:\n",
    "    if not cb.on_fit_begin(learner): return\n",
    "    print('Begin')\n",
    "    for epoch in range(epochs):\n",
    "        print('Training...')\n",
    "        cb.on_epoch_begin(epoch)\n",
    "        all_batches(learner.data.train_dl, cb)\n",
    "        cb.on_epoch_end()\n",
    "        \n",
    "        if cb.begin_validate():\n",
    "            print('Validating...')\n",
    "            with torch.no_grad():\n",
    "                cb.on_epoch_begin(epoch)\n",
    "                all_batches(learner.data.val_dl, cb)\n",
    "                cb.on_epoch_end()\n",
    "\n",
    "        if cb.do_stop() or not cb.on_epoch_end(): break\n",
    "    cb.on_fit_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset from /home/mayen/Learn/Kaggle/toxic_comment/data/488f084da443568790e2d337ae61163f8e6a3e0cdbb87cdc7ed7a0108936e241.pklb\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = RawDataset(project_dirname, subsample=10000)\n",
    "processed_dataset = ProcessedDataset(raw_dataset, overwrite=False)\n",
    "trainable_dataset = TrainableDataset(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "params = {'batch_size': 128, 'num_workers': 3, 'drop_last': False, 'collate_fn': collate}\n",
    "\n",
    "transform = Transformer('comment_words', 'b_target')\n",
    "train_ds = CommentDataset(trainable_dataset.train_df, transform)\n",
    "val_ds = CommentDataset(trainable_dataset.val_df, transform)\n",
    "test_ds = CommentDataset(trainable_dataset.test_df, transform)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, shuffle=True, **params)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, shuffle=False, **params)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, shuffle=False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'vocab_size': transform.vocab.size,\n",
    "    'embedding_size': 64,\n",
    "    'padding_idx': transform.vocab[transform.vocab.PAD],\n",
    "    'nhead': 4,\n",
    "    'd_model': 128,\n",
    "    'width': 3\n",
    "}\n",
    "model = SimpleModel(model_params).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "data_bunch = DataLoaderBunch(train_dl, val_dl, test_dl)\n",
    "\n",
    "model_learner = Learner(model, optimizer, loss, data_bunch)\n",
    "callbacks = CallbackHandler([Recorder()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0 (8 sec): loss: 0.26054 val loss: 0.22936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 (8 sec): loss: 0.22199 val loss: 0.22735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 (8 sec): loss: 0.22181 val loss: 0.22619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 3 (8 sec): loss: 0.22136 val loss: 0.22378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 4 (8 sec): loss: 0.21641 val loss: 0.21064\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model_learner.fit_one_epoch(epoch)\n",
    "    model_learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a7ff25bb4548bfb62c8a82bb47a41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4736, in get_value\n    return libindex.get_value_box(s, key)\n  File \"pandas/_libs/index.pyx\", line 51, in pandas._libs.index.get_value_box\n  File \"pandas/_libs/index.pyx\", line 47, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-87ae24501f27>\", line 54, in __getitem__\n    X, Y = self.transform(sample)\n  File \"<ipython-input-6-87ae24501f27>\", line 21, in __call__\n    Y = self._column_selector(sample, self.Y_cols)\n  File \"<ipython-input-6-87ae24501f27>\", line 10, in _column_selector\n    return df[columns]\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/series.py\", line 1068, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4744, in get_value\n    raise e1\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4730, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 88, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'b_target'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-42aa0fb65cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-938ec9977552>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4736, in get_value\n    return libindex.get_value_box(s, key)\n  File \"pandas/_libs/index.pyx\", line 51, in pandas._libs.index.get_value_box\n  File \"pandas/_libs/index.pyx\", line 47, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-87ae24501f27>\", line 54, in __getitem__\n    X, Y = self.transform(sample)\n  File \"<ipython-input-6-87ae24501f27>\", line 21, in __call__\n    Y = self._column_selector(sample, self.Y_cols)\n  File \"<ipython-input-6-87ae24501f27>\", line 10, in _column_selector\n    return df[columns]\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/series.py\", line 1068, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4744, in get_value\n    raise e1\n  File \"/home/mayen/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 4730, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 88, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'b_target'\n"
     ]
    }
   ],
   "source": [
    "model_learner.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_learner.recorder.total_loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
