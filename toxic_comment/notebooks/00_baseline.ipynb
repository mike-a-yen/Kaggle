{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "from typing import Callable, List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dirname = Path('../').resolve()\n",
    "raw_data_dirname = project_dirname / 'data/raw'\n",
    "data_files = list(raw_data_dirname.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_df_sha(df: pd.DataFrame) -> str:\n",
    "    id_str = ''.join(map(str, df.id))\n",
    "    id_bstr = id_str.encode()\n",
    "    sha = hashlib.sha256(id_bstr)\n",
    "    return sha.hexdigest()\n",
    "\n",
    "    \n",
    "class RawDataset:\n",
    "    def __init__(self, project_dirname:Path, subsample: int = None) -> None:\n",
    "        self.subsample = subsample\n",
    "        self.project_dirname = project_dirname\n",
    "        self.data_dirname = self.project_dirname / 'data'\n",
    "        self.raw_data_dirname = self.data_dirname / 'raw'\n",
    "        self.train_filename = self.raw_data_dirname / 'train.csv'\n",
    "        self.test_filename = self.raw_data_dirname / 'test.csv'\n",
    "        \n",
    "        assert self.train_filename.exists()\n",
    "        assert self.test_filename.exists()\n",
    "\n",
    "        self.train_df = pd.read_csv(self.train_filename)\n",
    "        self.test_df = pd.read_csv(self.test_filename)\n",
    "        \n",
    "        if self.subsample != 0:\n",
    "            self.train_df = self.train_df.iloc[0: self.subsample]\n",
    "        self.identifier = compute_df_sha(self.train_df)\n",
    "\n",
    "class ProcessedDataset:\n",
    "    toxicity_subtypes = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    identity_attributes = [\n",
    "        'male', 'female', 'transgender', 'other_gender',\n",
    "        'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation',\n",
    "        'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion',\n",
    "        'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
    "        'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
    "        'other_disability'\n",
    "    ]\n",
    "    _binarize_columns = ['target'] + toxicity_subtypes\n",
    "    binary_columns = [f'b_{name}' for name in _binarize_columns]\n",
    "    \n",
    "    def __init__(self, raw_dataset: RawDataset, overwrite: bool = False) -> None:\n",
    "        self.identifier = raw_dataset.identifier\n",
    "        self.project_dirname = raw_dataset.project_dirname\n",
    "        self.data_dirname = raw_dataset.data_dirname\n",
    "        self.cache_path = self.data_dirname / f'{self.identifier}.pklb'\n",
    "        \n",
    "        if self.cache_path.exists() and not overwrite:\n",
    "            print(f'Loading processed dataset from {self.cache_path}')\n",
    "            self.load()\n",
    "        else:\n",
    "            self.train_df = raw_dataset.train_df\n",
    "            self.test_df = raw_dataset.test_df\n",
    "            self.featurize()\n",
    "            self.save()\n",
    "\n",
    "    def featurize(self) -> None:\n",
    "        print('Featurizing....')\n",
    "        self._prepare_df_labels(self.train_df)\n",
    "        self._prepare_features(self.train_df)\n",
    "        self._prepare_features(self.test_df)\n",
    "        print('Done.')\n",
    "            \n",
    "    def _prepare_df_labels(self, df: pd.DataFrame) -> None:\n",
    "        for column, new_column in zip(self._binarize_columns, self.binary_columns):\n",
    "            df[new_column] = df[column].apply(self._binarize_label)\n",
    "\n",
    "    def _prepare_features(self, df: pd.DataFrame) -> None:\n",
    "        df['comment_words'] = df.comment_text.apply(\n",
    "                lambda x: [w.text for w in tokenizer(x)]\n",
    "            )\n",
    "\n",
    "        \n",
    "    def _binarize_label(self, target: float) -> int:\n",
    "        \"\"\"According to competition rules, target values >= 0.5 are considered the positive class.\"\"\"\n",
    "        return int(target >= 0.5)\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        cache_data = {\n",
    "            'train_df': self.train_df,\n",
    "            'test_df': self.test_df\n",
    "        }\n",
    "        with open(self.cache_path, 'wb') as fw:\n",
    "            pickle.dump(cache_data, fw)\n",
    "        \n",
    "    def load(self) -> None:\n",
    "        with open(self.cache_path, 'rb') as fo:\n",
    "            cache_data = pickle.load(fo)\n",
    "        self.train_df = cache_data['train_df']\n",
    "        self.test_df = cache_data['test_df']\n",
    "        \n",
    "    \n",
    "\n",
    "def split_df(df: pd.DataFrame, frac: float = 0.1) -> Tuple[pd.DataFrame]:\n",
    "    n_val = int(np.ceil(df.shape[0]*frac))\n",
    "    df = df.sample(frac=1.)\n",
    "    val_df = df.iloc[0: n_val]\n",
    "    train_df = df.iloc[n_val:]\n",
    "    assert val_df.shape[0] + train_df.shape[0] == df.shape[0]\n",
    "    return train_df, val_df\n",
    "    \n",
    "class TrainableDataset:\n",
    "    def __init__(self, processed_dataset: ProcessedDataset) -> None:\n",
    "        self.trainval_df = processed_dataset.train_df\n",
    "        self.train_df, self.val_df = split_df(self.trainval_df, frac=0.1)\n",
    "        self.test_df = processed_dataset.test_df\n",
    "        \n",
    "        self.n_train = self.train_df.shape[0]\n",
    "        self.n_val = self.val_df.shape[0]\n",
    "        self.n_test = self.test_df.shape[0]\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Train samples: {self.n_train}, Val samples: {self.n_val}, Test samples: {self.n_test}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD = \"<PAD>\"\n",
    "    BOS = \"<BOS>\"\n",
    "    EOS = \"<EOS>\"\n",
    "    BOT = \"<BOT>\"\n",
    "    EOT = \"<EOT>\"\n",
    "    UNK = \"<UNK>\"\n",
    "    \n",
    "    specials = [PAD, BOS, EOS, BOT, EOT, UNK]\n",
    "    vocab = []\n",
    "    token_to_int = dict()\n",
    "    int_to_token = dict()\n",
    "    \n",
    "    def __init__(self, tokens: list = []) -> None:\n",
    "        [self.add_token(t) for t in self.specials]\n",
    "        [self.add_token(t) for t in tokens]\n",
    "        self.UNK_IDX = self.vocab.index(self.UNK)\n",
    "            \n",
    "    def add_token(self, token) -> None:\n",
    "        if token in self.vocab:\n",
    "            return\n",
    "        else:\n",
    "            idx = len(self.vocab)\n",
    "            self.token_to_int[token] = idx\n",
    "            self.int_to_token[idx] = token\n",
    "            self.vocab.append(token)\n",
    "            \n",
    "    def __getitem__(self, token: str) -> int:\n",
    "        return self.token_to_int.get(token, self.UNK_IDX)\n",
    "    \n",
    "    def __contains__(self, token: str) -> bool:\n",
    "        return token in self.vocab\n",
    "    \n",
    "    def get(self, x: Union[str, int], reverse: bool = False) -> int:\n",
    "        if reverse:\n",
    "            return self.int_to_token.get(x, self.UNK)\n",
    "        else:\n",
    "            return self.token_to_int.get(x, self.UNK_IDX)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_to_int)\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self)\n",
    "    \n",
    "    \n",
    "class VocabEncoder:\n",
    "    def __init__(self, vocab: Vocabulary) -> None:\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def _encode_token(self, token: str) -> int:\n",
    "        encoded = []\n",
    "        if Vocabulary.BOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOT])\n",
    "        encoded += [self.vocab[c] for c in token]\n",
    "        if Vocabulary.EOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOT])\n",
    "        return encoded\n",
    "        \n",
    "    def encode(self, seq: List[str]):\n",
    "        encoded = []\n",
    "        if Vocabulary.BOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOS])\n",
    "        for token in seq:\n",
    "            encoded += self._encode_token(token)\n",
    "        if Vocabulary.EOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOS])\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, seq: List[int]) -> List[str]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, X_cols: Union[list, str], Y_cols: Union[list, str] = None) -> None:\n",
    "        self.X_cols = X_cols\n",
    "        self.Y_cols = Y_cols\n",
    "        self.vocab = Vocabulary(string.printable)\n",
    "        self.encoder = VocabEncoder(self.vocab)\n",
    "        assert 'a' in self.vocab\n",
    "\n",
    "    def _column_selector(self, df: pd.DataFrame, columns: Union[list, str]) -> pd.Series:\n",
    "        return df[columns]\n",
    "    \n",
    "    def _vocab_encoder(self, seq: List[str]) -> pd.Series:\n",
    "        return encode_sequence(seq, self.vocab)\n",
    "\n",
    "    def __call__(self, sample: pd.Series) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X = self._column_selector(sample, self.X_cols)\n",
    "        X = self.encoder.encode(X)\n",
    "        # maxlen = max(map(len, X))\n",
    "        # X = pad_sequences(X, maxlen=maxlen, value=self.vocab[self.vocab.PAD])\n",
    "        if self.Y_cols is not None:\n",
    "            Y = self._column_selector(sample, self.Y_cols)\n",
    "        else:\n",
    "            Y = None\n",
    "        X = torch.LongTensor(X)\n",
    "        Y = torch.Tensor([Y])\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "class BatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int, shuffle=False) -> None:\n",
    "        self.N = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def _index_sampler(self) -> None:\n",
    "        if self.shuffle:\n",
    "            self.idxs = torch.randperm(self.N).tolist()\n",
    "        else:\n",
    "            self.idxs = torch.arange(self.N).tolist()\n",
    "        \n",
    "    def __iter__(self) -> List[int]:\n",
    "        self._index_sampler()\n",
    "        for i in range(0, self.N, self.batch_size):\n",
    "            yield self.idxs[i: i+self.batch_size]\n",
    "\n",
    "\n",
    "class CommentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Callable) -> None:\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.df.iloc[idx]\n",
    "        X, Y = self.transform(sample)\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "def collate(batch: List[tuple]) -> Tuple[torch.Tensor]:\n",
    "    X, Y = zip(*batch)\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    Y = torch.stack(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderBunch:\n",
    "    def __init__(self, train_dl: torch.utils.data.DataLoader, val_dl: torch.utils.data.DataLoader, test_dl: torch.utils.data.DataLoader = None, c = None) -> None:\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c\n",
    "    \n",
    "    @property\n",
    "    def train_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.train_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def val_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.val_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def test_ds(self) -> torch.utils.data.Dataset:\n",
    "        if self.test_dl is not None:\n",
    "            return self.test_dl.dataset\n",
    "        else:\n",
    "            raise ValueError('No test dataloader available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, model_params: dict) -> None:\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            model_params['vocab_size'],\n",
    "            model_params['embedding_size'],\n",
    "            padding_idx=model_params['padding_idx'],\n",
    "        )\n",
    "        self.conv_layer = nn.Conv1d(\n",
    "            model_params['embedding_size'],\n",
    "            model_params['d_model'],\n",
    "            model_params['width'],\n",
    "            \n",
    "        )\n",
    "        self.conv_pool_layer = nn.MaxPool1d(\n",
    "            model_params['width'],\n",
    "            stride=model_params['width'] - 1\n",
    "        )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            model_params['d_model'],\n",
    "            model_params['nhead'],\n",
    "            model_params['d_model']\n",
    "        )\n",
    "        self.transformer_pool_layer = nn.AdaptiveMaxPool1d(1)\n",
    "        self.logit_layer = nn.Linear(model_params['d_model'], 1)\n",
    "        self.conv_act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, token_ids) -> torch.Tensor:\n",
    "        emb = self.embedding_layer(token_ids)\n",
    "        emb_channels_first = torch.transpose(emb, 1, 2)\n",
    "        conv = self.conv_layer(emb_channels_first)\n",
    "        conv_pool = self.conv_pool_layer(conv)\n",
    "        conv_pool = torch.transpose(conv_pool, 1, 2)\n",
    "        conv_pool = self.conv_act(conv_pool)\n",
    "        encoded = self.transformer_layer(conv_pool)\n",
    "        encoded = torch.transpose(encoded, 1, 2)\n",
    "        encoded_pool = self.transformer_pool_layer(encoded)\n",
    "        encoded_pool = encoded_pool[..., 0]\n",
    "        logit = self.logit_layer(encoded_pool)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy_with_logits(y_hat, y_true):\n",
    "    pred = y_hat >= 0\n",
    "    truth = y_true >= 0.5\n",
    "    acc = (pred==truth).float().mean()\n",
    "    return acc\n",
    "\n",
    "    \n",
    "class Learner:\n",
    "    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.modules.loss, data: DataLoaderBunch) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = list(model.parameters())[0].device\n",
    "        self.data = data\n",
    "    \n",
    "class Tester(Learner):\n",
    "    def __init__(self, learner: nn.Module, data: DataLoaderBunch) -> None:\n",
    "        self.learner = learner\n",
    "        self.data = data\n",
    "        self.recorder = Recorder()\n",
    "        self.callbacks = CallbackHandler([self.recorder])\n",
    "        self.callbacks.learner = learner\n",
    "        \n",
    "    def evaluate(self) -> None:\n",
    "        self.callbacks.begin_validate()\n",
    "        with torch.no_grad():\n",
    "            with self._pbar(self.data.test_dl) as pbar:\n",
    "                for batch in self.data.test_dl:\n",
    "                    X, Y = self._convert_batch(*batch)\n",
    "                    y_hat = self.learner.model(X)\n",
    "                    batch_size = y_hat.shape[0]\n",
    "                    cost = self.learner.loss_fn(y_hat, Y) * batch_size\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vvxi25xjf2ex'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_string(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length: int) -> str:\n",
    "    return ''.join(map(str, np.random.choice(list(string.ascii_lowercase+string.digits), length)))\n",
    "\n",
    "class Callback:\n",
    "    def set_run(self, cb) -> bool:\n",
    "        self.run = cb\n",
    "        return True\n",
    "\n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        return True\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_begin(self, epoch) -> bool:\n",
    "        self.epoch = epoch\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.__class__.__name__.lower()\n",
    "\n",
    "\n",
    "class CallbackHandler:\n",
    "    def __init__(self, callbacks: List[Callback] = [], id: str = None) -> None:\n",
    "        self.identifier = id if id else random_string(12)\n",
    "        self.callbacks = []\n",
    "        [self.add(cb) for cb in callbacks]\n",
    "        \n",
    "    def __call__(self, fn_name: str, *args, **kwargs) -> bool:\n",
    "        res = True\n",
    "        for cb in sorted(self.callbacks, key=lambda x: x._order):\n",
    "            fn = getattr(cb, fn_name, None)\n",
    "            res = res and fn(*args, **kwargs)\n",
    "        return res\n",
    "        \n",
    "    def add(self, cb: Callback) -> None:\n",
    "        if cb in self.callbacks:\n",
    "            return\n",
    "        else:\n",
    "            setattr(self, cb.name, cb)\n",
    "            self.callbacks.append(cb)\n",
    "            \n",
    "    def set_run(self) -> bool:\n",
    "        return self('set_run', self)\n",
    "\n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        self.in_train = True\n",
    "        learner.stop = False\n",
    "        return self('on_fit_begin', learner)\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        res = not self.in_train\n",
    "        return self('on_fit_end')\n",
    "        \n",
    "    \n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        self.learner.model.train()\n",
    "        return self('on_epoch_begin', epoch)\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        return self('on_epoch_end')\n",
    "        \n",
    "    def begin_validate(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        return self('begin_validate')\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        return self('end_validate')\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        return self('begin_test')\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        return self('end_test')\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        return self('on_batch_begin', X, Y)\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        return self('on_loss_begin', y_hat)\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        return self('on_loss_end', loss)\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return self('on_backward_begin')\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        return self('on_backward_end')\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        return self('on_step_begin')\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        return self('on_step_end')\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        return self('on_batch_end')\n",
    "    \n",
    "    def do_stop(self) -> bool:\n",
    "        try:\n",
    "            return self.learner.stop\n",
    "        finally:\n",
    "            self.learner.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    _order = 0\n",
    "    def __init__(self) -> None:\n",
    "        self.n_epochs = 0.\n",
    "        self.records = dict(\n",
    "            loss=[], val_loss=[], \n",
    "            acc=[], val_acc=[],\n",
    "            correct=[], val_correct=[],\n",
    "            train_time=[], val_time=[],\n",
    "            train_samples=[], val_samples=[],\n",
    "        )\n",
    "        self.test_records = dict(loss=None, val_loss=None, test_time=None, test_samples=None)\n",
    "\n",
    "    def on_fit_begin(self, learner) -> bool:\n",
    "        super(Recorder, self).on_fit_begin(learner)\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        return True\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        super(Recorder, self).on_epoch_begin(epoch)\n",
    "        self._reset_state()\n",
    "        return True\n",
    "        \n",
    "    def on_batch_begin(self, X, Y) -> bool:\n",
    "        self.Y = Y.cpu().numpy()\n",
    "        self.y_true = self.Y >= 0.5\n",
    "        self.batch_size = Y.shape[0]\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat) -> bool:\n",
    "        self.y_hat = y_hat.detach().cpu().numpy()\n",
    "        self.y_pred = self.y_hat >= 0.\n",
    "        self.total_correct += (self.y_pred==self.y_true).sum()\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss) -> bool:\n",
    "        self.total_loss += loss.item() * self.batch_size\n",
    "        self.total_samples += self.batch_size\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return self.in_train\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        super(Recorder, self).on_epoch_end()\n",
    "        self._log_metrics()\n",
    "        if self.in_train:\n",
    "            self.n_epochs += 1\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        super(Recorder, self).begin_validate()\n",
    "        self.in_train = False\n",
    "        self._reset_state()\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_train = True\n",
    "        print(f'{self._display_latest_metrics()}')\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        super(Recorder, self).begin_test()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_test = False\n",
    "        return True\n",
    "    \n",
    "    def _log_metrics(self) -> None:\n",
    "        elapsed_time = time.time() - self.time_start\n",
    "        self.total_loss /= self.total_samples\n",
    "        self.total_acc = self.total_correct/self.total_samples\n",
    "        if self.in_train:\n",
    "            self.records['loss'].append(self.total_loss)\n",
    "            self.records['train_time'].append(elapsed_time)\n",
    "            self.records['train_samples'].append(self.total_samples)\n",
    "            self.records['correct'].append(self.total_correct)\n",
    "            self.records['acc'].append(self.total_acc)\n",
    "        elif self.in_test:\n",
    "            self.records['test_loss'] = self.total_loss\n",
    "            self.records['test_time'] = elapsed_time\n",
    "            self.records['test_samples'] = self.total_samples\n",
    "        else:\n",
    "            self.records['val_loss'].append(self.total_loss)\n",
    "            self.records['val_time'].append(elapsed_time)\n",
    "            self.records['val_samples'].append(self.total_samples)\n",
    "            self.records['val_correct'].append(self.total_correct)\n",
    "            self.records['val_acc'].append(self.total_acc)\n",
    "    \n",
    "    def _reset_state(self) -> None:\n",
    "        self.total_loss = 0.\n",
    "        self.total_correct = 0.\n",
    "        self.total_samples = 0\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "    def _display_latest_metrics(self) -> str:\n",
    "        total_time = int(self.records[\"train_time\"][-1] + self.records[\"val_time\"][-1])\n",
    "        epoch = f'Epoch {self.epoch} ({total_time} sec):'\n",
    "        train_metrics = f'loss = {self.records[\"loss\"][-1]:0.5f} acc = {100 * self.records[\"acc\"][-1]:0.2f}%'\n",
    "        val_metrics = f'val loss = {self.records[\"val_loss\"][-1]:0.5f} val_acc = {100 * self.records[\"val_acc\"][-1]:0.2f}%'\n",
    "        return f'{epoch} {train_metrics} {val_metrics}'\n",
    "    \n",
    "class Checkpointer(Callback):\n",
    "    _order = 1\n",
    "    def end_validate(self) -> bool:\n",
    "        n_epochs = self.run.recorder.n_epochs\n",
    "        metrics = self.run.recorder.records\n",
    "        val_loss = metrics['val_loss'][-1]\n",
    "        if val_loss == min(metrics['val_loss']):\n",
    "            save_name = f'epoch={int(n_epochs)}-val_loss={val_loss:0.5f}.pth'\n",
    "            save_path = self.checkpoint_dir / save_name\n",
    "            self.save_model(save_path)\n",
    "        return True\n",
    "        \n",
    "    def save_model(self, path: Union[str, Path]) -> bool:\n",
    "        torch.save(self.learner.model.state_dict(), path)\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_dir(self) -> Path:\n",
    "        dirname = Path(f'checkpoints/{self.run.identifier}').resolve()\n",
    "        dirname.mkdir(exist_ok=True, parents=True)\n",
    "        return dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training callback routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    def __init__(self, learner: Learner, cb: CallbackHandler) -> None:\n",
    "        self.learner = learner\n",
    "        self.recorder = Recorder()\n",
    "        self.cb = cb\n",
    "        self.cb.add(self.recorder)\n",
    "        self.cb.add(Checkpointer())\n",
    "        self.device = self.learner.device\n",
    "        \n",
    "    def one_batch(self, X: torch.Tensor, Y: torch.Tensor) -> None:\n",
    "        X, Y = self._convert_batch((X, Y))\n",
    "        if not self.cb.on_batch_begin(X, Y): return\n",
    "        y_hat = self.learner.model(X)\n",
    "        if not self.cb.on_loss_begin(y_hat): return\n",
    "        loss = self.learner.loss_fn(y_hat, Y)\n",
    "        self.cb.on_loss_end(loss)\n",
    "        if not self.cb.on_backward_begin(): return\n",
    "        loss.backward()  # get grads\n",
    "        self.cb.on_backward_end()\n",
    "        self.cb.on_step_begin()\n",
    "        self.learner.optimizer.step()  # apply grads\n",
    "        self.cb.on_step_end()\n",
    "        self.learner.optimizer.zero_grad()  # zero grads\n",
    "        self.cb.on_batch_end()\n",
    "        \n",
    "    def all_batches(self, dataloader: torch.utils.data.DataLoader) -> None:\n",
    "        with self.pbar(dataloader) as pbar:\n",
    "            for batch in dataloader:\n",
    "                self.one_batch(*batch)\n",
    "                pbar.update(self.recorder.batch_size)\n",
    "        \n",
    "    def _fit_one_epoch(self, epoch: int = None) -> None:\n",
    "        self.cb.on_epoch_begin(epoch)\n",
    "        dataloader = self.learner.data.train_dl\n",
    "        self.all_batches(dataloader)\n",
    "        self.cb.on_epoch_end()\n",
    "        return \n",
    "    \n",
    "    def fit(self, epochs: int) -> None:\n",
    "        self.cb.set_run()\n",
    "        self.cb.on_fit_begin(self.learner)\n",
    "        for epoch in range(epochs):\n",
    "            self._fit_one_epoch(epoch)\n",
    "            self.validate()\n",
    "        return\n",
    "                \n",
    "    def validate(self) -> None:\n",
    "        self.cb.begin_validate()\n",
    "        dataloader = self.learner.data.val_dl\n",
    "        with torch.no_grad():\n",
    "            self.all_batches(dataloader)\n",
    "        self.cb.end_validate()\n",
    "        return\n",
    "\n",
    "    def pbar(self, dataloader: torch.utils.data.DataLoader, **kwargs) -> tqdm_notebook:\n",
    "        params = {\n",
    "            'total': len(dataloader.dataset),\n",
    "            'unit': 'samples',\n",
    "            'leave': False\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "        return tqdm_notebook(**params)\n",
    "        \n",
    "    \n",
    "    def _convert_batch(self, batch: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"Send batch data to the model's device\"\"\"\n",
    "        return tuple(x.to(self.device) for x in batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset from /home/myen/Learn/Kaggle/toxic_comment/data/488f084da443568790e2d337ae61163f8e6a3e0cdbb87cdc7ed7a0108936e241.pklb\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = RawDataset(project_dirname, subsample=10000)\n",
    "processed_dataset = ProcessedDataset(raw_dataset, overwrite=False)\n",
    "trainable_dataset = TrainableDataset(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "params = {'batch_size': 128, 'num_workers': 3, 'drop_last': False, 'collate_fn': collate}\n",
    "\n",
    "transform = Transformer('comment_words', 'b_target')\n",
    "train_ds = CommentDataset(trainable_dataset.train_df, transform)\n",
    "val_ds = CommentDataset(trainable_dataset.val_df, transform)\n",
    "test_ds = CommentDataset(trainable_dataset.test_df, transform)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, shuffle=True, **params)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, shuffle=False, **params)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, shuffle=False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'vocab_size': transform.vocab.size,\n",
    "    'embedding_size': 64,\n",
    "    'padding_idx': transform.vocab[transform.vocab.PAD],\n",
    "    'nhead': 8,\n",
    "    'd_model': 256,\n",
    "    'width': 3\n",
    "}\n",
    "model = SimpleModel(model_params).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "data_bunch = DataLoaderBunch(train_dl, val_dl, test_dl)\n",
    "\n",
    "model_learner = Learner(model, optimizer, loss, data_bunch)\n",
    "fitter = Fitter(model_learner, CallbackHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0 (6 sec): loss = 0.24114 acc = 92.93% val loss = 0.19790 val_acc = 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 (6 sec): loss = 0.21886 acc = 94.13% val loss = 0.19735 val_acc = 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 (6 sec): loss = 0.20413 acc = 94.13% val loss = 0.18957 val_acc = 95.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d6cb057a8b4aa19670e104506282e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitter.fit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Recorder' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-a93fd8e9696d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Recorder' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "fitter.cb.recorder.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
