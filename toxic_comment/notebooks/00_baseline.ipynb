{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "from typing import Callable, List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dirname = Path('../').resolve()\n",
    "raw_data_dirname = project_dirname / 'data/raw'\n",
    "data_files = list(raw_data_dirname.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df_sha(df: pd.DataFrame) -> str:\n",
    "    id_str = ''.join(map(str, df.id))\n",
    "    id_bstr = id_str.encode()\n",
    "    sha = hashlib.sha256(id_bstr)\n",
    "    return sha.hexdigest()\n",
    "\n",
    "    \n",
    "class RawDataset:\n",
    "    def __init__(self, project_dirname:Path, subsample: int = None) -> None:\n",
    "        self.subsample = subsample\n",
    "        self.project_dirname = project_dirname\n",
    "        self.data_dirname = self.project_dirname / 'data'\n",
    "        self.raw_data_dirname = self.data_dirname / 'raw'\n",
    "        self.train_filename = self.raw_data_dirname / 'train.csv'\n",
    "        self.test_filename = self.raw_data_dirname / 'test.csv'\n",
    "        \n",
    "        assert self.train_filename.exists()\n",
    "        assert self.test_filename.exists()\n",
    "        \n",
    "        self.train_df = pd.read_csv(self.train_filename)\n",
    "        self.test_df = pd.read_csv(self.test_filename)\n",
    "        \n",
    "        if self.subsample != 0:\n",
    "            self.train_df = self.train_df.iloc[0: self.subsample]\n",
    "        self.identifier = compute_df_sha(self.train_df)\n",
    "\n",
    "class ProcessedDataset:\n",
    "    toxicity_subtypes = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    identity_attributes = [\n",
    "        'male', 'female', 'transgender', 'other_gender',\n",
    "        'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation',\n",
    "        'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion',\n",
    "        'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
    "        'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
    "        'other_disability'\n",
    "    ]\n",
    "    _binarize_columns = ['target'] + toxicity_subtypes\n",
    "    binary_columns = [f'b_{name}' for name in _binarize_columns]\n",
    "    \n",
    "    def __init__(self, raw_dataset: RawDataset, overwrite: bool = False) -> None:\n",
    "        self.identifier = raw_dataset.identifier\n",
    "        self.project_dirname = raw_dataset.project_dirname\n",
    "        self.data_dirname = raw_dataset.data_dirname\n",
    "        self.cache_path = self.data_dirname / f'{self.identifier}.pklb'\n",
    "        \n",
    "        if self.cache_path.exists() and not overwrite:\n",
    "            print(f'Loading processed dataset from {self.cache_path}')\n",
    "            self.load()\n",
    "        else:\n",
    "            self.train_df = raw_dataset.train_df\n",
    "            self.test_df = raw_dataset.test_df\n",
    "            self.featurize()\n",
    "            self.save()\n",
    "\n",
    "    def featurize(self) -> None:\n",
    "        print('Featurizing....')\n",
    "        self._prepare_df_labels(self.train_df)\n",
    "        self._prepare_features(self.train_df)\n",
    "        self._prepare_features(self.test_df)\n",
    "        print('Done.')\n",
    "            \n",
    "    def _prepare_df_labels(self, df: pd.DataFrame) -> None:\n",
    "        for column, new_column in zip(self._binarize_columns, self.binary_columns):\n",
    "            df[new_column] = df[column].apply(self._binarize_label)\n",
    "\n",
    "    def _prepare_features(self, df: pd.DataFrame) -> None:\n",
    "        df['comment_words'] = df.comment_text.apply(\n",
    "                lambda x: [w.text for w in tokenizer(x)]\n",
    "            )\n",
    "\n",
    "        \n",
    "    def _binarize_label(self, target: float) -> int:\n",
    "        \"\"\"According to competition rules, target values >= 0.5 are considered the positive class.\"\"\"\n",
    "        return int(target >= 0.5)\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        cache_data = {\n",
    "            'train_df': self.train_df,\n",
    "            'test_df': self.test_df\n",
    "        }\n",
    "        with open(self.cache_path, 'wb') as fw:\n",
    "            pickle.dump(cache_data, fw)\n",
    "        \n",
    "    def load(self) -> None:\n",
    "        with open(self.cache_path, 'rb') as fo:\n",
    "            cache_data = pickle.load(fo)\n",
    "        self.train_df = cache_data['train_df']\n",
    "        self.test_df = cache_data['test_df']\n",
    "        \n",
    "    \n",
    "\n",
    "def split_df(df: pd.DataFrame, frac: float = 0.1) -> Tuple[pd.DataFrame]:\n",
    "    n_val = int(np.ceil(df.shape[0]*frac))\n",
    "    df = df.sample(frac=1.)\n",
    "    val_df = df.iloc[0: n_val]\n",
    "    train_df = df.iloc[n_val:]\n",
    "    assert val_df.shape[0] + train_df.shape[0] == df.shape[0]\n",
    "    return train_df, val_df\n",
    "    \n",
    "class TrainableDataset:\n",
    "    def __init__(self, processed_dataset: ProcessedDataset) -> None:\n",
    "        self.trainval_df = processed_dataset.train_df\n",
    "        self.train_df, self.val_df = split_df(self.trainval_df, frac=0.1)\n",
    "        self.test_df = processed_dataset.test_df\n",
    "        \n",
    "        self.n_train = self.train_df.shape[0]\n",
    "        self.n_val = self.val_df.shape[0]\n",
    "        self.n_test = self.test_df.shape[0]\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Train samples: {self.n_train}, Val samples: {self.n_val}, Test samples: {self.n_test}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD = \"<PAD>\"\n",
    "    BOS = \"<BOS>\"\n",
    "    EOS = \"<EOS>\"\n",
    "    BOT = \"<BOT>\"\n",
    "    EOT = \"<EOT>\"\n",
    "    UNK = \"<UNK>\"\n",
    "    \n",
    "    specials = [PAD, BOS, EOS, BOT, EOT, UNK]\n",
    "    vocab = []\n",
    "    token_to_int = dict()\n",
    "    int_to_token = dict()\n",
    "    \n",
    "    def __init__(self, tokens: list = []) -> None:\n",
    "        [self.add_token(t) for t in self.specials]\n",
    "        [self.add_token(t) for t in tokens]\n",
    "        self.UNK_IDX = self.vocab.index(self.UNK)\n",
    "            \n",
    "    def add_token(self, token) -> None:\n",
    "        if token in self.vocab:\n",
    "            return\n",
    "        else:\n",
    "            idx = len(self.vocab)\n",
    "            self.token_to_int[token] = idx\n",
    "            self.int_to_token[idx] = token\n",
    "            self.vocab.append(token)\n",
    "            \n",
    "    def __getitem__(self, token: str) -> int:\n",
    "        return self.token_to_int.get(token, self.UNK_IDX)\n",
    "    \n",
    "    def __contains__(self, token: str) -> bool:\n",
    "        return token in self.vocab\n",
    "    \n",
    "    def get(self, x: Union[str, int], reverse: bool = False) -> int:\n",
    "        if reverse:\n",
    "            return self.int_to_token.get(x, self.UNK)\n",
    "        else:\n",
    "            return self.token_to_int.get(x, self.UNK_IDX)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_to_int)\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self)\n",
    "    \n",
    "    \n",
    "class VocabEncoder:\n",
    "    def __init__(self, vocab: Vocabulary) -> None:\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def _encode_token(self, token: str) -> int:\n",
    "        encoded = []\n",
    "        if Vocabulary.BOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOT])\n",
    "        encoded += [self.vocab[c] for c in token]\n",
    "        if Vocabulary.EOT in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOT])\n",
    "        return encoded\n",
    "        \n",
    "    def encode(self, seq: List[str]):\n",
    "        encoded = []\n",
    "        if Vocabulary.BOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.BOS])\n",
    "        for token in seq:\n",
    "            encoded += self._encode_token(token)\n",
    "        if Vocabulary.EOS in self.vocab:\n",
    "            encoded.append(self.vocab[self.vocab.EOS])\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, seq: List[int]) -> List[str]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, X_cols: Union[list, str], Y_cols: Union[list, str] = None) -> None:\n",
    "        self.X_cols = X_cols\n",
    "        self.Y_cols = Y_cols\n",
    "        self.vocab = Vocabulary(string.printable)\n",
    "        self.encoder = VocabEncoder(self.vocab)\n",
    "        assert 'a' in self.vocab\n",
    "\n",
    "    def _column_selector(self, df: pd.DataFrame, columns: Union[list, str]) -> pd.Series:\n",
    "        return df[columns]\n",
    "    \n",
    "    def _vocab_encoder(self, seq: List[str]) -> pd.Series:\n",
    "        return encode_sequence(seq, self.vocab)\n",
    "\n",
    "    def __call__(self, sample: pd.Series) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X = self._column_selector(sample, self.X_cols)\n",
    "        X = self.encoder.encode(X)\n",
    "        # maxlen = max(map(len, X))\n",
    "        # X = pad_sequences(X, maxlen=maxlen, value=self.vocab[self.vocab.PAD])\n",
    "        if self.Y_cols is not None:\n",
    "            Y = self._column_selector(sample, self.Y_cols)\n",
    "        else:\n",
    "            Y = None\n",
    "        X = torch.LongTensor(X)\n",
    "        Y = torch.Tensor([Y])\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "class BatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int, shuffle=False) -> None:\n",
    "        self.N = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def _index_sampler(self) -> None:\n",
    "        if self.shuffle:\n",
    "            self.idxs = torch.randperm(self.N).tolist()\n",
    "        else:\n",
    "            self.idxs = torch.arange(self.N).tolist()\n",
    "        \n",
    "    def __iter__(self) -> List[int]:\n",
    "        self._index_sampler()\n",
    "        for i in range(0, self.N, self.batch_size):\n",
    "            yield self.idxs[i: i+self.batch_size]\n",
    "\n",
    "\n",
    "class CommentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Callable) -> None:\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.df.iloc[idx]\n",
    "        X, Y = self.transform(sample)\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "def collate(batch: List[tuple]) -> Tuple[torch.Tensor]:\n",
    "    X, Y = zip(*batch)\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    Y = torch.stack(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderBunch:\n",
    "    def __init__(self, train_dl: torch.utils.data.DataLoader, val_dl: torch.utils.data.DataLoader, test_dl: torch.utils.data.DataLoader = None, c = None) -> None:\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c\n",
    "    \n",
    "    @property\n",
    "    def train_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.train_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def val_ds(self) -> torch.utils.data.Dataset:\n",
    "        return self.val_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def test_ds(self) -> torch.utils.data.Dataset:\n",
    "        if self.test_dl is not None:\n",
    "            return self.test_dl.dataset\n",
    "        else:\n",
    "            raise ValueError('No test dataloader available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, model_params: dict) -> None:\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            model_params['vocab_size'],\n",
    "            model_params['embedding_size'],\n",
    "            padding_idx=model_params['padding_idx'],\n",
    "        )\n",
    "        self.conv_layer = nn.Conv1d(\n",
    "            model_params['embedding_size'],\n",
    "            model_params['d_model'],\n",
    "            model_params['width'],\n",
    "            \n",
    "        )\n",
    "        self.conv_pool_layer = nn.MaxPool1d(\n",
    "            model_params['width'],\n",
    "            stride=model_params['width'] - 1\n",
    "        )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            model_params['d_model'],\n",
    "            model_params['nhead'],\n",
    "            model_params['d_model']\n",
    "        )\n",
    "        self.transformer_pool_layer = nn.AdaptiveMaxPool1d(1)\n",
    "        self.logit_layer = nn.Linear(model_params['d_model'], 1)\n",
    "        self.conv_act = nn.Tanh()\n",
    "        \n",
    "    def forward(self, token_ids) -> torch.Tensor:\n",
    "        emb = self.embedding_layer(token_ids)\n",
    "        emb_channels_first = torch.transpose(emb, 1, 2)\n",
    "        conv = self.conv_layer(emb_channels_first)\n",
    "        conv_pool = self.conv_pool_layer(conv)\n",
    "        conv_pool = torch.transpose(conv_pool, 1, 2)\n",
    "        conv_pool = self.conv_act(conv_pool)\n",
    "        encoded = self.transformer_layer(conv_pool)\n",
    "        encoded = torch.transpose(encoded, 1, 2)\n",
    "        encoded_pool = self.transformer_pool_layer(encoded)\n",
    "        encoded_pool = encoded_pool[..., 0]\n",
    "        logit = self.logit_layer(encoded_pool)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy_with_logits(y_hat, y_true):\n",
    "    pred = y_hat >= 0\n",
    "    truth = y_true >= 0.5\n",
    "    acc = (pred==truth).float().mean()\n",
    "    return acc\n",
    "\n",
    "    \n",
    "class Learner:\n",
    "    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.modules.loss, data: DataLoaderBunch) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = list(model.parameters())[0].device\n",
    "        self.data = data\n",
    "        self.recorder = Recorder()\n",
    "        self.callbacks = CallbackHandler([self.recorder])\n",
    "        \n",
    "    def one_batch(self, X: torch.Tensor, Y: torch.Tensor) -> None:\n",
    "        X, Y = self._convert_batch((X, Y))\n",
    "        if not self.callbacks.on_batch_begin(X, Y): return\n",
    "        y_hat = self.model(X)\n",
    "        if not self.callbacks.on_loss_begin(y_hat): return\n",
    "        loss = self.loss_fn(y_hat, Y)\n",
    "        self.callbacks.on_loss_end(loss)\n",
    "        if not self.callbacks.on_backward_begin(): return\n",
    "        loss.backward()  # get grads\n",
    "        self.callbacks.on_backward_end()\n",
    "        self.callbacks.on_step_begin()\n",
    "        self.optimizer.step()  # apply grads\n",
    "        self.callbacks.on_step_end()\n",
    "        self.optimizer.zero_grad()  # zero grads\n",
    "        self.callbacks.on_batch_end()\n",
    "        \n",
    "    def fit_one_epoch(self, epoch: int = None) -> None:\n",
    "        if epoch == 0: self.callbacks.on_fit_begin(self)\n",
    "        self.callbacks.on_epoch_begin(epoch)\n",
    "        with self.pbar(self.data.train_dl) as pbar:\n",
    "            for batch in self.data.train_dl:\n",
    "                self.one_batch(*batch)\n",
    "                batch_size = self.recorder.batch_size\n",
    "                pbar.update(batch_size)\n",
    "        self.callbacks.on_epoch_end()\n",
    "        return \n",
    "                \n",
    "    def validate(self) -> None:\n",
    "        self.callbacks.begin_validate()\n",
    "        with torch.no_grad():\n",
    "            with self.pbar(self.data.val_dl) as pbar:\n",
    "                for batch in self.data.val_dl:\n",
    "                    self.one_batch(*batch)\n",
    "                    batch_size = self.recorder.batch_size\n",
    "                    pbar.update(batch_size)\n",
    "        self.callbacks.end_validate()\n",
    "        return\n",
    "\n",
    "    def pbar(self, dataloader: torch.utils.data.DataLoader, **kwargs) -> tqdm_notebook:\n",
    "        params = {\n",
    "            'total': len(dataloader.dataset),\n",
    "            'unit': 'samples',\n",
    "            'leave': False\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "        return tqdm_notebook(**params)\n",
    "        \n",
    "    \n",
    "    def _convert_batch(self, batch: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"Send batch data to the model's device\"\"\"\n",
    "        return tuple(x.to(self.device) for x in batch)\n",
    "    \n",
    "class Tester(Learner):\n",
    "    def __init__(self, learner: nn.Module, data: DataLoaderBunch) -> None:\n",
    "        self.learner = learner\n",
    "        self.data = data\n",
    "        self.recorder = Recorder()\n",
    "        self.callbacks = CallbackHandler([self.recorder])\n",
    "        self.callbacks.learner = learner\n",
    "        \n",
    "    def evaluate(self) -> None:\n",
    "        self.callbacks.begin_validate()\n",
    "        with torch.no_grad():\n",
    "            with self._pbar(self.data.test_dl) as pbar:\n",
    "                for batch in self.data.test_dl:\n",
    "                    X, Y = self._convert_batch(*batch)\n",
    "                    y_hat = self.learner.model(X)\n",
    "                    batch_size = y_hat.shape[0]\n",
    "                    cost = self.learner.loss_fn(y_hat, Y) * batch_size\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        return True\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_begin(self, epoch) -> bool:\n",
    "        self.epoch = epoch\n",
    "        return True\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "class CallbackHandler:\n",
    "    def __init__(self, callbacks: List[Callback] = []) -> None:\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "    def on_fit_begin(self, learner: Learner) -> bool:\n",
    "        self.learner = learner\n",
    "        self.in_train = True\n",
    "        learner.stop = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_fit_begin(learner)\n",
    "        return res\n",
    "    \n",
    "    def on_fit_end(self) -> bool:\n",
    "        res = not self.in_train\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.after_fit()\n",
    "        return res\n",
    "    \n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        self.learner.model.train()\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_epoch_begin(epoch)\n",
    "        return res\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_epoch_end()\n",
    "        return res\n",
    "        \n",
    "    def begin_validate(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.begin_validate()\n",
    "        return res\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.end_validate()\n",
    "        return res\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        self.learner.model.eval()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.begin_test()\n",
    "        return res\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self.learner.model.train()\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.end_test()\n",
    "        return res\n",
    "    \n",
    "    def on_batch_begin(self, X: torch.Tensor, Y: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_batch_begin(X, Y)\n",
    "        return res\n",
    "    \n",
    "    def on_loss_begin(self, y_hat: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_loss_begin(y_hat)\n",
    "        return res\n",
    "    \n",
    "    def on_loss_end(self, loss: torch.Tensor) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_loss_end(loss)\n",
    "        return res\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_backward_begin()\n",
    "        return res\n",
    "    \n",
    "    def on_backward_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_backward_end()\n",
    "        return res\n",
    "    \n",
    "    def on_step_begin(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_step_begin()\n",
    "        return res\n",
    "    \n",
    "    def on_step_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_step_end()\n",
    "        return res\n",
    "    \n",
    "    def on_batch_end(self) -> bool:\n",
    "        res = True\n",
    "        for cb in self.callbacks:\n",
    "            res = res and cb.on_batch_end()\n",
    "        return res\n",
    "    \n",
    "    def do_stop(self) -> bool:\n",
    "        try:\n",
    "            return self.learner.stop\n",
    "        finally:\n",
    "            self.learner.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    def __init__(self) -> None:\n",
    "        self.records = dict(\n",
    "            loss=[], val_loss=[], \n",
    "            train_time=[], val_time=[],\n",
    "            train_samples=[], val_samples=[],\n",
    "        )\n",
    "        self.test_records = dict(loss=None, val_loss=None, test_time=None, test_samples=None)\n",
    "\n",
    "    def on_fit_begin(self, learner) -> bool:\n",
    "        super(Recorder, self).on_fit_begin(learner)\n",
    "        self.in_train = True\n",
    "        self.in_test = False\n",
    "        return True\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int) -> bool:\n",
    "        super(Recorder, self).on_epoch_begin(epoch)\n",
    "        self._reset_state()\n",
    "        return True\n",
    "        \n",
    "    def on_batch_begin(self, X, Y) -> bool:\n",
    "        self.batch_size = Y.shape[0]\n",
    "        return True\n",
    "    \n",
    "    def on_loss_begin(self, y_hat) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def on_loss_end(self, loss) -> bool:\n",
    "        self.total_loss += loss.item() * self.batch_size\n",
    "        self.total_samples += self.batch_size\n",
    "        return True\n",
    "    \n",
    "    def on_backward_begin(self) -> bool:\n",
    "        return self.in_train\n",
    "    \n",
    "    def on_epoch_end(self) -> bool:\n",
    "        super(Recorder, self).on_epoch_end()\n",
    "        self._log_metrics()\n",
    "        return True\n",
    "    \n",
    "    def begin_validate(self) -> bool:\n",
    "        super(Recorder, self).begin_validate()\n",
    "        self.in_train = False\n",
    "        self._reset_state()\n",
    "        return True\n",
    "    \n",
    "    def end_validate(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_train = True\n",
    "        print(f'{self._display_latest_metrics()}')\n",
    "        return True\n",
    "    \n",
    "    def begin_test(self) -> bool:\n",
    "        super(Recorder, self).begin_test()\n",
    "        self.in_train = False\n",
    "        self.in_test = True\n",
    "        return True\n",
    "    \n",
    "    def end_test(self) -> bool:\n",
    "        self._log_metrics()\n",
    "        self.in_test = False\n",
    "        return True\n",
    "    \n",
    "    def _log_metrics(self) -> None:\n",
    "        elapsed_time = time.time() - self.time_start\n",
    "        self.total_loss /= self.total_samples\n",
    "        if self.in_train:\n",
    "            self.records['loss'].append(self.total_loss)\n",
    "            self.records['train_time'].append(elapsed_time)\n",
    "            self.records['train_samples'].append(self.total_samples)\n",
    "        elif self.in_test:\n",
    "            self.records['test_loss'] = self.total_loss\n",
    "            self.records['test_time'] = elapsed_time\n",
    "            self.records['test_samples'] = self.total_samples\n",
    "        else:\n",
    "            self.records['val_loss'].append(self.total_loss)\n",
    "            self.records['val_time'].append(elapsed_time)\n",
    "            self.records['val_samples'].append(self.total_samples)\n",
    "    \n",
    "    def _reset_state(self) -> None:\n",
    "        self.total_loss = 0.\n",
    "        self.total_samples = 0\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "    def _display_latest_metrics(self) -> str:\n",
    "        total_time = int(self.records[\"train_time\"][-1] + self.records[\"val_time\"][-1])\n",
    "        epoch = f'Epoch {self.epoch} ({total_time} sec):'\n",
    "        train_loss = f'loss = {self.records[\"loss\"][-1]:0.5f}'\n",
    "        val_loss = f'val loss = {self.records[\"val_loss\"][-1]:0.5f}'\n",
    "        return f'{epoch} {train_loss} {val_loss}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training callback routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def one_batch(X: torch.Tensor, Y:torch.Tensor, cb: CallbackHandler) -> None:\n",
    "    if not cb.on_batch_begin(X, Y): return\n",
    "    y_hat = cb.learner.model(X)\n",
    "    if not cb.on_loss_begin(y_hat): return\n",
    "    loss = cb.learner.loss_fn(cb.learner.model(X), Y)\n",
    "    cb.on_loss_end(loss)\n",
    "    if not cb.on_backward_begin(): return\n",
    "    loss.backward()\n",
    "    cb.on_backward_end()\n",
    "    cb.on_step_begin()\n",
    "    cb.learner.optimizer.step()\n",
    "    cb.on_step_end()\n",
    "    cb.learner.optimizer.zero_grad()\n",
    "    cb.on_batch_end()\n",
    "        \n",
    "def all_batches(dataloader: torch.utils.data.DataLoader, cb) -> None:\n",
    "    for X, Y in dataloader:\n",
    "        one_batch(X, Y, cb)\n",
    "        if cb.do_stop(): return\n",
    "        \n",
    "def fit(epochs: int, learner: Learner, cb: CallbackHandler) -> None:\n",
    "    if not cb.on_fit_begin(learner): return\n",
    "    print('Begin')\n",
    "    for epoch in range(epochs):\n",
    "        print('Training...')\n",
    "        cb.on_epoch_begin(epoch)\n",
    "        all_batches(learner.data.train_dl, cb)\n",
    "        cb.on_epoch_end()\n",
    "        \n",
    "        if cb.begin_validate():\n",
    "            print('Validating...')\n",
    "            with torch.no_grad():\n",
    "                cb.on_epoch_begin(epoch)\n",
    "                all_batches(learner.data.val_dl, cb)\n",
    "                cb.on_epoch_end()\n",
    "\n",
    "        if cb.do_stop() or not cb.on_epoch_end(): break\n",
    "    cb.on_fit_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset from /home/mayen/Learn/Kaggle/toxic_comment/data/b99b7e3dc5efe6cd58de07884a1829563aa094f60141b25554fff4bef39a23c5.pklb\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = RawDataset(project_dirname, subsample=0)\n",
    "processed_dataset = ProcessedDataset(raw_dataset, overwrite=False)\n",
    "trainable_dataset = TrainableDataset(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "params = {'batch_size': 128, 'num_workers': 3, 'drop_last': False, 'collate_fn': collate}\n",
    "\n",
    "transform = Transformer('comment_words', 'b_target')\n",
    "train_ds = CommentDataset(trainable_dataset.train_df, transform)\n",
    "val_ds = CommentDataset(trainable_dataset.val_df, transform)\n",
    "test_ds = CommentDataset(trainable_dataset.test_df, transform)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, shuffle=True, **params)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, shuffle=False, **params)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, shuffle=False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'vocab_size': transform.vocab.size,\n",
    "    'embedding_size': 64,\n",
    "    'padding_idx': transform.vocab[transform.vocab.PAD],\n",
    "    'nhead': 8,\n",
    "    'd_model': 256,\n",
    "    'width': 3\n",
    "}\n",
    "model = SimpleModel(model_params).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "data_bunch = DataLoaderBunch(train_dl, val_dl, test_dl)\n",
    "\n",
    "model_learner = Learner(model, optimizer, loss, data_bunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0 (2429 sec): loss = 0.17616 val loss = 0.16182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 (2428 sec): loss = 0.16285 val loss = 0.17784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 (2423 sec): loss = 0.15912 val loss = 0.15402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 3 (2424 sec): loss = 0.15667 val loss = 0.15989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 4 (2424 sec): loss = 0.15560 val loss = 0.15877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 5 (2424 sec): loss = 0.15484 val loss = 0.15806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 6 (2423 sec): loss = 0.15341 val loss = 0.14906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 7 (2423 sec): loss = 0.15257 val loss = 0.16862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 8 (2424 sec): loss = 0.15433 val loss = 0.15378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 9 (2423 sec): loss = 0.15283 val loss = 0.18161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 10 (2422 sec): loss = 0.15254 val loss = 0.15113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 11 (2411 sec): loss = 0.15025 val loss = 0.15723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 12 (2412 sec): loss = 0.15061 val loss = 0.15226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1624386), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6a27c170e64e01ac4ab7131cb303e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180488), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model_learner.fit_one_epoch(epoch)\n",
    "    model_learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(raw_dataset.raw_data_dirname / 'train.csv', iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804869</td>\n",
       "      <td>6333967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Maybe the tax on \"things\" would be collected w...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399385</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804870</td>\n",
       "      <td>6333969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>What do you call people who STILL think the di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399528</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804871</td>\n",
       "      <td>6333982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>thank you ,,,right or wrong,,, i am following ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399457</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804872</td>\n",
       "      <td>6334009</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>Anyone who is quoted as having the following e...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399519</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804873</td>\n",
       "      <td>6334010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Students defined as EBD are legally just as di...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>399318</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1804874 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    target                                       comment_text  \\\n",
       "0          59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1          59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2          59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3          59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4          59856  0.893617               haha you guys are a bunch of losers.   \n",
       "...          ...       ...                                                ...   \n",
       "1804869  6333967  0.000000  Maybe the tax on \"things\" would be collected w...   \n",
       "1804870  6333969  0.000000  What do you call people who STILL think the di...   \n",
       "1804871  6333982  0.000000  thank you ,,,right or wrong,,, i am following ...   \n",
       "1804872  6334009  0.621212  Anyone who is quoted as having the following e...   \n",
       "1804873  6334010  0.000000  Students defined as EBD are legally just as di...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "0               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "2               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "3               0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "4               0.021277  0.000000         0.021277  0.872340     0.0    0.0   \n",
       "...                  ...       ...              ...       ...     ...    ...   \n",
       "1804869         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804870         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804871         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "1804872         0.030303  0.030303         0.045455  0.621212     0.0    NaN   \n",
       "1804873         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
       "\n",
       "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "1            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "2            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "3            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "4            0.0  ...        2006  rejected      0    0    0      1         0   \n",
       "...          ...  ...         ...       ...    ...  ...  ...    ...       ...   \n",
       "1804869      NaN  ...      399385  approved      0    0    0      0         0   \n",
       "1804870      NaN  ...      399528  approved      0    0    0      0         0   \n",
       "1804871      NaN  ...      399457  approved      0    0    0      0         0   \n",
       "1804872      NaN  ...      399519  approved      0    0    0      0         0   \n",
       "1804873      NaN  ...      399318  approved      0    0    0      0         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0                    0.0                         0                         4  \n",
       "1                    0.0                         0                         4  \n",
       "2                    0.0                         0                         4  \n",
       "3                    0.0                         0                         4  \n",
       "4                    0.0                         4                        47  \n",
       "...                  ...                       ...                       ...  \n",
       "1804869              0.0                         0                         4  \n",
       "1804870              0.0                         0                         4  \n",
       "1804871              0.0                         0                         4  \n",
       "1804872              0.0                         0                        66  \n",
       "1804873              0.0                         0                         4  \n",
       "\n",
       "[1804874 rows x 45 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = reader.get_chunk()\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-701065872cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "reader.get_chunk(10000)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
