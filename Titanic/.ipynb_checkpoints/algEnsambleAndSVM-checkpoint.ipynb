{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic = pandas.read_csv('data_sets/train.csv')\n",
    "titanic_test = pandas.read_csv(\"data_sets/test.csv\")\n",
    "\n",
    "# Missing data\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')\n",
    "# Non-numeric data\n",
    "titanic.loc[titanic['Sex']=='male', 'Sex'] = 0\n",
    "titanic.loc[titanic['Sex']=='female', 'Sex'] = 1\n",
    "\n",
    "embarkedDict = {'Q': 2, 'C': 1, 'S': 0}\n",
    "for key in embarkedDict:\n",
    "    titanic.loc[titanic['Embarked'] == key, 'Embarked'] = embarkedDict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Missing data\n",
    "titanic_test['Age'] = titanic_test['Age'].fillna(titanic['Age'].median())\n",
    "titanic_test['Fare'] = titanic_test['Fare'].fillna(titanic['Fare'].median())\n",
    "# Non-numeric data\n",
    "titanic_test.loc[titanic_test['Sex']=='male', 'Sex'] = 0\n",
    "titanic_test.loc[titanic_test['Sex']=='female', 'Sex'] = 1\n",
    "\n",
    "for key in embarkedDict:\n",
    "    titanic_test.loc[titanic_test['Embarked'] == key, 'Embarked'] = embarkedDict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family size and NameLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Mlle          2\n",
      "Col           2\n",
      "Don           1\n",
      "Capt          1\n",
      "Ms            1\n",
      "Countess      1\n",
      "Sir           1\n",
      "Mme           1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use univariate feature selection. This essentially goes column by column, and figures out which columns correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "As usual, sklearn has a function that will help us with feature selection, SelectKBest. This selects the best features from the data, and allows us to specify how many it selects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEoCAYAAAB4oxv+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzP8HRL9Aj0oVtD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a8c9690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolinesofiatti/anaconda/envs/py33/lib/python3.3/site-packages/IPython/kernel/__main__.py:39: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "dtype: int64\n",
      "{'Hirvonen1': 411, 'Sawyer0': 554, 'Shorney0': 92, 'McCarthy0': 7, 'Mallet2': 656, 'Graham0': 699, 'Graham1': 242, 'Van Impe2': 361, 'Connors0': 113, 'Harmer0': 634, 'Montvila0': 698, 'Cann0': 37, 'Andersson0': 137, 'Andersson6': 14, 'Slayter0': 290, 'Futrelle1': 4, 'Hood0': 70, 'Persson1': 241, 'McKane0': 342, 'Murdlin0': 491, 'Hodges0': 586, 'Goldschmidt0': 93, 'Chaffee1': 89, 'Pain0': 343, 'Kimball1': 513, 'McGowan0': 23, 'Shellard0': 423, 'Nicola-Yarred1': 39, 'Myhrman0': 626, 'Heininen0': 655, 'Theobald0': 612, 'Isham0': 163, 'Seward0': 383, 'Hogeboom1': 618, 'Sjostedt0': 212, 'Jermyn0': 322, \"O'Dwyer0\": 28, 'Naidenoff0': 259, 'Cardeza1': 556, 'Henry0': 239, 'Boulos0': 497, 'Andrew0': 135, 'Gee0': 397, 'Woolner0': 55, 'Sagesser0': 528, 'Peter2': 120, 'Jonsson0': 477, 'Simonius-Blumer0': 531, 'Dahl0': 299, 'Touma2': 232, 'Sheerlinck0': 79, 'Jensen1': 584, 'Jensen0': 527, 'Coxon0': 91, 'Petterson1': 379, 'Appleton2': 478, 'Jonkoff0': 609, 'Reynaldo0': 380, 'Baclini3': 384, 'Larsson0': 211, 'Lulic0': 659, 'Pears1': 141, 'Drew2': 358, 'Carter3': 340, 'Carter1': 226, 'Laitinen0': 427, 'Rood0': 169, 'Humblen0': 570, 'Garside0': 481, 'Harper1': 52, 'Campbell0': 401, 'Marvin1': 604, 'Weir0': 567, 'Bing0': 72, 'Nysten0': 132, 'Landergren0': 328, 'Lindqvist1': 543, 'Lines1': 677, 'Slabenoff0': 499, 'Funk0': 313, 'McCormack0': 661, 'Mudd0': 671, 'Ibrahim Shawah0': 643, 'Farrell0': 445, 'Cunningham0': 355, 'Jussila1': 110, 'Reuchlin0': 660, 'Klasen2': 161, 'Williams-Lambert0': 308, 'Collyer2': 216, 'Van der hoef0': 158, 'Kilgannon0': 629, 'Hakkarainen1': 133, 'Perkin0': 194, 'Dick1': 563, 'Guggenheim0': 636, 'Carrau0': 81, 'Klaber0': 578, 'Doling1': 95, 'Gavey0': 511, 'Herman3': 510, 'Lobb1': 230, 'Louch1': 373, 'Molson0': 418, 'Goodwin7': 59, 'Denkoff0': 297, 'Gale1': 348, 'Banfield0': 696, 'Coleff0': 435, 'de Messemaeker1': 469, 'Lemberopolous0': 673, 'Hunt0': 218, 'Young0': 291, 'Smith0': 160, 'Stone0': 662, 'Staneff0': 74, 'Johnston3': 633, 'Nasser1': 10, 'Lang0': 431, 'Badt0': 541, 'Romaine0': 171, 'Harris1': 62, 'Burke0': 134, 'Glynn0': 32, 'Hippach1': 294, 'Andrews1': 249, 'Andrews0': 647, 'Kantor1': 96, 'Pickard0': 370, 'Culumovic0': 674, 'Connaghton0': 605, 'Faunthorpe1': 53, 'Mayne0': 577, 'Leinonen0': 526, 'Rugg0': 56, 'Razi0': 679, 'Bishop1': 263, 'Hays2': 658, 'Greenberg0': 579, 'Hays0': 279, 'Novel0': 57, 'Zimmerman0': 364, 'Moran0': 6, 'Moran1': 106, 'Bazzani0': 200, 'Meo0': 142, 'Otter0': 640, 'Hoyt1': 206, 'Risien0': 453, 'Healy0': 248, 'Abbing0': 675, 'Davies0': 336, 'Bryhl1': 590, 'Davies2': 460, 'Salkjelsvik0': 103, 'Maioni0': 428, 'Plotcharsky0': 335, 'Giles1': 681, 'Cavendish1': 600, 'Marechal0': 669, 'Leader0': 641, 'Leitch0': 496, 'Holverson1': 35, 'Sedgwick0': 302, 'Chronopoulos1': 71, 'Beane1': 456, 'Berriman0': 594, 'Boulos2': 131, 'Coutts2': 305, 'Coleridge0': 220, 'Drazenoic0': 122, 'Eitemiller0': 538, 'Gilinski0': 490, 'Olsson0': 254, 'Hanna0': 268, 'Madsen0': 119, 'Lennon1': 46, 'Holm0': 657, 'Mockler0': 315, 'Garfirth0': 614, 'Keefe0': 404, 'Foo0': 529, 'del Carlo1': 316, 'Elias0': 624, 'McGough0': 433, 'Leonard0': 165, 'Samaan2': 48, 'Taylor1': 547, 'Hampe0': 378, 'Vander Cruyssen0': 689, 'Lurette0': 178, 'Ball0': 293, 'Emanuel0': 628, 'Kraeff0': 42, 'Johnson2': 9, 'Kallio0': 374, 'Patchett0': 480, 'Gaskell0': 637, 'Windelov0': 417, 'Gillespie0': 585, 'Lundahl0': 522, 'Kent0': 415, 'Allen0': 5, 'Bradley0': 430, 'Nakid2': 333, 'Williams1': 145, 'Corn0': 147, 'Dahlberg0': 695, 'Hegarty0': 536, 'Cumings1': 2, 'Mellinger1': 246, 'Willey0': 532, 'Foreman0': 387, 'Butt0': 451, 'Goncalves0': 400, 'Nosworthy0': 51, 'Partner0': 295, 'Tobin0': 627, 'Backstrom1': 188, 'Calderhead0': 575, 'Saad0': 566, 'Sandstrom2': 11, 'Asplund6': 25, 'Peuchen0': 385, 'Strandberg0': 407, 'Tornquist0': 245, 'Cook0': 546, 'Becker3': 167, 'Slocovski0': 85, 'Nenkoff0': 205, 'Bostandyeff0': 519, 'Wilhelms0': 551, 'de Pelsmaeker0': 255, 'Hocking3': 449, 'Hocking4': 625, 'de Mulder0': 258, 'Masselmani0': 20, 'Ford4': 84, 'Asim0': 318, 'Johannesen-Bratthammer0': 381, 'Barber0': 262, 'Eklund0': 617, 'Ahlin1': 40, 'Levy0': 264, 'Maisner0': 399, 'Yousif0': 310, 'Fry0': 654, 'Horgan0': 508, 'Lahoud0': 443, \"O'Leary0\": 535, 'Bystrom0': 684, 'Flynn0': 369, 'Navratil2': 138, 'Allison3': 269, 'Perreault0': 441, 'Bourke2': 172, 'Duran y More1': 685, 'Skoog5': 63, 'Mernagh0': 179, 'Watson0': 552, 'Ridsdale0': 446, 'Dakic0': 560, 'Mineff0': 266, 'Davison1': 304, 'Bonnell0': 12, 'Stranden0': 602, 'Lindahl0': 223, 'Mullens0': 569, 'Nicholson0': 458, 'Backstrom3': 83, 'Dean3': 90, 'Fleming0': 275, 'Frauenthal2': 540, 'Mannion0': 589, 'Frauenthal1': 296, 'Lindell1': 503, 'Birkeland0': 351, 'Bailey0': 611, 'Meek0': 357, 'Gheorgheff0': 362, 'Lefebre4': 162, 'Cherry0': 234, 'van Melkebeke0': 687, 'Richards5': 376, 'Vande Velde0': 608, 'Richards2': 350, 'Daly0': 432, 'Attalah0': 111, 'Wells2': 606, 'Colley0': 542, 'Thomas1': 645, 'Watt0': 151, 'Newell2': 539, 'Newell1': 197, 'Murphy1': 219, 'Smart0': 402, 'Leeni0': 464, 'Hagland1': 386, 'Compton2': 665, 'Fortune5': 27, 'Harrison0': 238, 'Olsvigen0': 559, 'Pavlovic0': 440, 'Lesurer0': 596, 'Eustis1': 422, 'Tomlin0': 653, 'Byles0': 139, 'Bateman0': 140, 'Robins1': 124, 'Kalvik0': 534, 'Stankovic0': 257, 'Dowdell0': 77, 'Rekic0': 105, 'Shutes0': 506, 'Simmons0': 473, 'Vovk0': 442, 'Natsch1': 247, 'Lemore0': 437, 'Saalfeld0': 270, 'Richard0': 127, 'Matthews0': 360, 'McNamee1': 601, 'Haas0': 265, 'Douglas1': 457, 'Hoyt0': 638, 'Lester0': 651, 'Hansen2': 680, 'Rothschild1': 434, 'Astor1': 571, 'Ostby1': 54, 'Ohman0': 465, 'Crease0': 67, 'Hassab0': 558, 'Laleff0': 691, 'Frolicher-Stehli2': 489, 'Sjoblom0': 635, 'Pinsky0': 174, 'Fischer0': 561, 'Vanden Steen0': 311, 'Paulner0': 487, 'Lam0': 565, 'Mack0': 623, 'Longley0': 518, 'Ayoub0': 631, 'Duff Gordon1': 467, 'Ryerson4': 280, 'Green0': 204, 'Sadlier0': 338, 'Wright0': 466, \"O'Sullivan0\": 426, 'Beckwith2': 225, 'Jenkin0': 69, 'Morley0': 396, 'Milling0': 398, 'Nirva0': 615, 'Nankoff0': 598, 'Sharp0': 462, 'Penasco y Castellana1': 276, 'West3': 58, 'Weisz1': 125, 'Peters0': 557, 'Gronnestad0': 621, 'Coelho0': 123, 'van Billiard2': 143, 'Crosby2': 455, 'Cor0': 530, 'Sivola0': 159, 'Najib0': 690, 'Salonen0': 448, 'Uruchurtu0': 30, 'Carr0': 190, 'Svensson0': 424, \"O'Connell0\": 520, 'Barbara1': 317, 'Sinkkonen0': 603, 'Petranec0': 97, 'Strom1': 187, 'Strom2': 228, 'Elias2': 309, 'Stoytcheff0': 475, 'Artagaveytia0': 419, 'Sundman0': 356, 'Kink2': 68, 'Moutal0': 75, 'McCoy2': 272, 'Ekstrom0': 121, 'Aubart0': 323, 'McDermott0': 80, 'Toufik0': 450, 'Madill1': 562, 'Sivic0': 471, 'Wiseman0': 366, 'Millet0': 391, 'Lewy0': 267, 'Ponesell0': 644, 'Butler0': 544, 'Osen0': 129, 'Ilmakangas1': 591, 'Frost0': 412, 'Icard0': 61, 'Ilett0': 82, 'Morrow0': 470, 'Turkula0': 414, 'Parr0': 524, 'Davis0': 525, 'Caldwell2': 76, 'Hale0': 164, 'Somerton0': 416, 'Kelly0': 271, 'Betros0': 330, 'Panula5': 50, 'McMahon0': 118, 'Fahlstrom0': 210, 'Sage10': 149, 'Bowerman1': 312, 'Quick2': 429, 'Carlsson0': 610, 'Hassan0': 592, 'Christy2': 484, 'Duane0': 253, 'Vander Planke1': 19, 'Vander Planke2': 38, 'Bissette0': 243, 'Kink-Heilmann2': 168, 'Fox0': 303, 'Olsen0': 144, 'Olsen1': 180, 'Oreskovic0': 347, 'Beavan0': 326, 'Chibnall1': 155, 'Albimona0': 189, 'Alhomaki0': 670, 'Nysveen0': 292, 'Wiklund1': 325, 'Sloper0': 24, 'Leyson0': 213, 'Hart0': 353, 'Hart2': 283, 'Troupiansky0': 595, 'Mangan0': 620, 'Connolly0': 261, 'Karaic0': 504, 'Ling0': 157, 'Clifford0': 408, 'Yousseff0': 421, 'Harder1': 324, 'Warren1': 320, 'Emir0': 26, 'Baxter1': 114, 'Sobey0': 126, 'Slemen0': 652, 'Barah0': 616, 'Downton0': 485, 'Doharr0': 476, 'Webber0': 117, 'Hold1': 215, 'Andreasson0': 88, 'Elsbury0': 494, 'Kiernan1': 196, 'Mellors0': 208, 'Christmann0': 87, 'Chambers1': 587, 'Moore0': 116, 'Johanson0': 184, 'Robert1': 630, 'Abbott2': 252, 'Karlsson0': 410, 'Silvey1': 375, 'Pettersson0': 648, 'Soholt0': 580, 'Moubarek2': 65, 'Mamee0': 36, 'Sirayanian0': 60, 'Nilsson0': 284, 'Berglund0': 207, 'Keane0': 274, 'Chip0': 668, 'Francatelli0': 278, 'Givard0': 195, 'Barton0': 109, 'Cameron0': 193, 'Gilnagh0': 146, 'Youseff0': 185, 'LeRoy0': 452, 'Caram1': 482, 'Laroche3': 43, 'Nicholls2': 136, 'Lovell0': 209, 'Niskanen0': 345, 'Brewe0': 619, 'Behr0': 700, 'Hedman0': 646, 'Petroff0': 98, 'Stead0': 229, 'Stanley0': 420, 'Smiljanic0': 148, 'Devaney0': 44, 'Brown2': 548, 'Brown0': 177, 'Stephenson1': 493, 'Hosono0': 260, 'Scanlan0': 403, 'Rush0': 479, 'Norman0': 472, 'Johnson0': 273, 'Cribb1': 150, 'Arnold-Franchi1': 49, 'Reed0': 227, 'Barkworth0': 521, 'Silven2': 359, 'Hamalainen2': 224, 'Padro y Manent0': 459, 'Porter0': 107, 'Beesley0': 22, 'Peduzzi0': 389, 'Lahtinen2': 281, 'Canavan0': 425, 'Rice5': 17, 'Hansen1': 574, 'Roebling0': 686, 'Carbines0': 175, 'Parrish1': 236, 'Danoff0': 289, 'Pasic0': 666, 'Ringhini0': 327, 'Gallagher0': 573, 'Thorne0': 233, 'Celotti0': 86, 'Troutt0': 582, 'Lehmann0': 339, 'Sdycoff0': 352, 'Renouf3': 588, 'Renouf1': 409, 'Moussa0': 321, 'Adahl0': 319, 'Walker0': 436, 'Abelson1': 277, 'Long0': 632, \"O'Driscoll0\": 47, 'Williams0': 18, 'Dooley0': 701, 'Rouse0': 413, 'Charters0': 363, 'Endres0': 581, 'Widegren0': 349, 'Rommetvedt0': 545, 'Minahan1': 354, 'Jardin0': 507, 'McGovern0': 314, 'Johansson0': 100, 'Taussig2': 237, 'Mionoff0': 102, 'Parkes0': 251, 'Madigan0': 181, 'Ward0': 235, 'Greenfield1': 94, 'Thayer2': 461, 'Thorneycroft1': 372, \"O'Brien1\": 170, \"O'Brien0\": 463, 'Dorking0': 256, 'Kvillner0': 377, 'Serepeca0': 672, 'Palsson4': 8, 'Burns0': 298, 'Giglio0': 130, 'Vande Walle0': 183, 'Bowen0': 515, 'Goldsmith2': 154, 'Turja0': 555, 'Moen0': 73, 'Osman0': 642, 'Cleaver0': 576, 'Pengelly0': 217, 'Robbins0': 468, 'Collander0': 301, 'Rothes0': 613, 'Clarke1': 367, 'Markun0': 694, 'Spedden2': 287, 'Brocklebank0': 509, 'Balkic0': 688, 'Waelens0': 78, 'Andersen-Jensen1': 176, 'Augustsson0': 663, 'McEvoy0': 583, 'Todoroff0': 29, 'Gustafsson0': 331, 'Pekoniemi0': 112, 'Gustafsson2': 101, 'Frolicher2': 454, 'Cohen0': 186, \"O'Connor0\": 394, 'Hendekovic0': 282, 'Harrington0': 500, 'Karun1': 564, 'Chapman1': 495, 'Chapman0': 568, 'Trout0': 344, 'Sunderland0': 202, 'Hickman2': 115, 'Ivanoff0': 597, 'Harris0': 201, 'Reeves0': 240, 'Stewart0': 64, 'Widener2': 329, 'Goldenberg1': 388, 'Shelley1': 693, 'Sutehall0': 697, 'Wheadon0': 33, 'Hansen0': 514, 'Kirkland0': 517, 'Yrois0': 182, 'Braund1': 1, 'Swift0': 682, 'Blank0': 191, 'Danbom2': 365, 'Radeff0': 537, 'Aks1': 678, 'Homer0': 502, 'Tikkanen0': 334, 'Ali0': 192, 'Saundercock0': 13, 'Turpin1': 41, 'Edvardsson0': 553, 'Maenpaa0': 221, 'Bracken0': 203, 'Blackwell0': 300, 'Fynney0': 21, 'Baumann0': 156, 'Rosblom2': 231, 'Phillips0': 368, 'Gill0': 683, 'Kenyon1': 392, 'Dodge2': 382, 'Ryan0': 438, 'Moraweck0': 285, 'Nye0': 66, 'Buss0': 337, 'Spencer1': 31, 'Cacic0': 405, 'Odahl0': 307, 'Hawksford0': 599, 'Adams0': 346, 'Yasbeck1': 512, 'Daniel0': 505, 'Kassem0': 444, 'Mitchell0': 550, 'Sutton0': 516, 'Meyer1': 34, 'Meyer0': 649, 'White1': 99, 'Bjornstrom-Steffansson0': 371, 'Dennis0': 288, 'Bidois0': 332, 'Torber0': 501, 'Dimic0': 306, 'Jalsevac0': 390, 'Bengtsson0': 152, 'Heikkinen0': 3, 'Toomey0': 393, 'Moss0': 104, 'Markoff0': 676, 'Moor1': 607, 'Potter1': 692, 'Sirota0': 667, 'Angle1': 439, 'Allum0': 664, 'Jussila0': 483, 'Ross0': 486, 'Alexander0': 650, 'Jerwan0': 406, 'Stahelin-Maeglin0': 523, 'Hewlett0': 16, 'Knight0': 593, 'Jansson0': 341, 'Pernot0': 166, 'Minahan2': 222, 'Anderson0': 395, 'Jarvis0': 488, 'Lievens0': 622, 'Cairns0': 244, 'Mitkoff0': 533, 'Farthing0': 447, 'Harknett0': 214, 'Newsom2': 128, 'Wick2': 286, 'Meanwell0': 474, 'Honkanen0': 198, 'Turcin0': 173, 'Dantcheff0': 639, 'Davidson1': 549, 'Jacobsohn1': 199, 'Jacobsohn3': 498, 'Vestrom0': 15, 'Zabour1': 108, 'Rintamaki0': 492, 'Rogers0': 45, 'Lindblom0': 250, 'Silverthorne0': 572, 'Calic0': 153}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions = clf.predict(titanic_test[predictors])\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "submission.to_csv(\"svm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "predictions = predictions.astype(int)\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "submission.to_csv(\"algEnsamble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEW: Improving Submission\n",
    "\n",
    "- Parameter Tuning for ML algorithm\n",
    "- Generating new features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
